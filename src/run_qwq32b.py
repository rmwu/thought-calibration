"""
QwQ 32B

Inputs generated by `1-prepare_s1`
"""

import os
import json
import pickle
from datetime import datetime

from lmdeploy import pipeline, GenerationConfig, TurbomindEngineConfig
from lmdeploy import PytorchEngineConfig

from transformers import AutoTokenizer


tokenizer = AutoTokenizer.from_pretrained("Qwen/QwQ-32B")


def convert(messages):
    if type(messages) is str:
        messages = [
            {"role": "user", "content": messages}
        ]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    return text


if __name__ == "__main__":
    batch_size = 32

    HF_HOME = ""  # fill this in

    paths = [
        #"inputs/s1_truncated_qwq.json"
        #"inputs/s1_embed_qwq.json",

        #"inputs/math500-n=1.json",
        #"inputs/aime25-n=5.json",
        #"inputs/aime24-n=5.json",
        #"inputs/gpqa_diamond-n=1.json",
    ]

    for path in paths:
        assert os.path.exists(path), path

    engine_config = TurbomindEngineConfig(
        tp=4,
        max_batch_size=batch_size,
    )


    pipe = pipeline(f"{HF_HOME}/transformers/hub/models--Qwen--QwQ-32B/snapshots/976055f8c83f394f35dbd3ab09a285a984907bd0",
           backend_config=engine_config
    )

    seed = 0

    # first run vs. evals
    save_hidden = False
    if save_hidden:
        output_hidden = "generation"
    else:
        output_hidden = None

    # preprocessed vs. process here
    add_template = False

    gen_config = GenerationConfig(top_p=0.95,
                                  temperature=0.6,
                                  max_new_tokens=8192,
                                  output_last_hidden_state=output_hidden,
                                  random_seed=seed,
                                  do_sample=True)

    for fp_prompts in paths:
        with open(fp_prompts) as f:
            prompts = json.load(f)

        # preprocess
        if add_template:
            prompts = [convert(m) for m in prompts]

        save_batch_size = 128
        prompts_batched = [prompts[i:i+batch_size] for i in range(0, len(prompts), batch_size)]
        outputs = []
        if output_hidden:
            hidden_states = []
        output_paths = []
        # >>>
        idx = 0
        total = idx * save_batch_size // batch_size
        start = total
        print("working on", fp_prompts, len(prompts_batched))
        # <<<
        for batch in prompts_batched[start:]:
            results = pipe(batch, gen_config=gen_config,
                           do_preprocess=False)
            outputs.extend([x.text for x in results])
            if output_hidden:
                hidden_states.extend([x.last_hidden_state.float().cpu().numpy() for x in results])
            total += 1
            if len(outputs) >= save_batch_size:
                print(f"{datetime.now().strftime('%H:%M:%S')} {total*batch_size} / {len(prompts)}")
                fp_out = fp_prompts.replace(".json", f"-{idx}.json")
                fp_out = fp_out.replace("inputs/", "outputs-qwq/")
                with open(fp_out, "w+") as f:
                    json.dump(outputs, f)
                output_paths.append(fp_out)
                outputs = []

                if output_hidden:
                    fp_out = fp_prompts.replace(".json", f"-{idx}.pkl")
                    fp_out = fp_out.replace("inputs/", "outputs-qwq/")
                    with open(fp_out, "wb+") as f:
                        pickle.dump(hidden_states, f)
                    hidden_states = []

                idx += 1

        if output_hidden:
            fp_out = fp_prompts.replace(".json", f"-{idx}.pkl")
            fp_out = fp_out.replace("inputs/", "outputs-qwq/")
            with open(fp_out, "wb+") as f:
                pickle.dump(hidden_states, f)

        fp_out = fp_prompts.replace(".json", f"-{idx}.json")
        fp_out = fp_out.replace("inputs/", "outputs-qwq/")
        with open(fp_out, "w+") as f:
            json.dump(outputs, f)
        output_paths.append(fp_out)

        all_outputs = []
        for fp_out in output_paths:
            with open(fp_out) as f:
                all_outputs.extend(json.load(f))

        fp_out = fp_prompts.replace("inputs/", "outputs/")
        with open(fp_out, "w+") as f:
            json.dump(all_outputs, f)

        for fp_out in output_paths:
            os.system(f"rm {fp_out}")

