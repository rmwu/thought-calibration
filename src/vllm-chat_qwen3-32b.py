"""
Verifier prompts

Inputs generated by `1-prepare_s1`
"""

import os
import json
import pickle
from datetime import datetime

from vllm import LLM, SamplingParams

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-32B")


batch_size = 128
think = False


llm = LLM(model="Qwen/Qwen3-32B", tensor_parallel_size=4)
if think:
    sampling_params = SamplingParams(temperature=0.6, top_p=0.95,
                                     min_p=0, top_k=20,
                                     max_tokens=4096)
else:
    sampling_params = SamplingParams(temperature=0.7, top_p=0.80,
                                     min_p=0, top_k=20,
                                     max_tokens=4096)


paths = [
    #"inputs/s1_verify_qwq.json",
    #"inputs/s1_verify_llama3.3.json",
    #"inputs/s1_verify_qwen2.5.json",
    #"inputs/s1_consistent_llama3.3.json",
    #"inputs/s1_consistent_qwen2.5.json",
    #"inputs/s1_consistent_qwq.json",
    #"inputs/s1_leaf.json",
    #"inputs/s1_novel.json",
]

def convert(messages):
    if type(messages) is str:
        messages = [
            {"role": "user", "content": messages}
        ]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        enable_thinking=think,
    )
    return text

for fp_prompts in paths:
    with open(fp_prompts) as f:
        prompts = json.load(f)

    # preprocess
    prompts = [convert(m) for m in prompts]

    save_batch_size = 128
    prompts_batched = [prompts[i:i+batch_size] for i in range(0, len(prompts), batch_size)]
    outputs = []
    output_paths = []
    idx = 0
    # >>>
    total = idx * save_batch_size // batch_size
    start = total
    # <<<
    print(len(prompts_batched), start)
    for batch in prompts_batched[start:]:
        # >>>
        fp_out = fp_prompts.replace(".json", f"-{idx}.json")
        fp_out = fp_out.replace("inputs/", "outputs-qwen3/")
        if os.path.exists(fp_out):
            continue
        # <<<
        outputs.extend([x.outputs[0].text for x in llm.generate(batch,
            sampling_params)])
        total += 1
        if len(outputs) >= save_batch_size:
            print(f"{datetime.now().strftime('%H:%M:%S')} {total*batch_size} / {len(prompts)}")
            fp_out = fp_prompts.replace(".json", f"-{idx}.json")
            fp_out = fp_out.replace("inputs/", "outputs-qwen3/")
            with open(fp_out, "w+") as f:
                json.dump(outputs, f)
            output_paths.append(fp_out)
            outputs = []
            idx += 1

    fp_out = fp_prompts.replace(".json", f"-{idx}.json")
    fp_out = fp_out.replace("inputs/", "outputs-qwen3/")
    with open(fp_out, "w+") as f:
        json.dump(outputs, f)
    output_paths.append(fp_out)

    all_outputs = []
    for fp_out in output_paths:
        with open(fp_out) as f:
            all_outputs.extend(json.load(f))

    fp_out = fp_prompts.replace("inputs/", "outputs/")
    with open(fp_out, "w+") as f:
        json.dump(all_outputs, f)

    for fp_out in output_paths:
        os.system(f"rm {fp_out}")

