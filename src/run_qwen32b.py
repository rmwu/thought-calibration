"""
Qwen 32B

Inputs generated by `1-prepare_s1`
"""

import os
import json
import pickle
from datetime import datetime

from lmdeploy import pipeline, GenerationConfig, TurbomindEngineConfig
from lmdeploy import PytorchEngineConfig
from transformers import AutoTokenizer


tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-R1-Distill-Qwen-32B")


def convert(messages):
    if type(messages) is str:
        messages = [
            {"role": "user", "content": messages}
        ]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    return text


if __name__ == "__main__":
    batch_size = 32

    HF_HOME = ""  # fill this in

    paths = [
        #"inputs/s1_truncated_qwen2.5.json"
        #"inputs/s1_embed.json",

        #"inputs/math500-n=1.json",
        #"inputs/aime25-n=5.json",
        #"inputs/aime24-n=5.json",
        #"inputs/gpqa_diamond-n=1.json",
    ]


    for path in paths:
        assert os.path.exists(path), path

    engine_config = TurbomindEngineConfig(
        tp=4,
        max_batch_size=batch_size,
    )

    pipe = pipeline(f"{HF_HOME}/transformers/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-32B/snapshots/711ad2ea6aa40cfca18895e8aca02ab92df1a746",
           backend_config=engine_config
    )

    seed = 0

    # first run vs. evals
    save_hidden = False
    if save_hidden:
        output_hidden = "generation"
    else:
        output_hidden = None

    # preprocessed vs. process here
    add_template = False

    gen_config = GenerationConfig(top_p=0.9,
                                  temperature=0.6,
                                  max_new_tokens=8192,
                                  do_sample=True,
                                  output_last_hidden_state=output_hidden,
                                  random_seed=seed,
                                  )

    for fp_prompts in paths:
        with open(fp_prompts) as f:
            prompts = json.load(f)

        # preprocess
        if add_template:
            prompts = [convert(m) for m in prompts]

        save_batch_size = 128
        #save_batch_size = 512
        prompts_batched = [prompts[i:i+batch_size] for i in range(0, len(prompts), batch_size)]
        outputs = []
        if output_hidden:
            hidden_states = []
        output_paths = []
        total = 0
        idx = 0
        print("working on", fp_prompts, len(prompts_batched))
        for batch in prompts_batched:
            results = pipe(batch, gen_config=gen_config,
                           do_preprocess=False)
            outputs.extend([x.text for x in results])
            if output_hidden:
                hidden_states.extend([x.last_hidden_state.float().cpu().numpy() for x in results])
            total += 1
            if len(outputs) >= save_batch_size:
                print(f"{datetime.now().strftime('%H:%M:%S')} {total*batch_size} / {len(prompts)}")
                fp_out = fp_prompts.replace(".json", f"-{idx}.json")
                fp_out = fp_out.replace("inputs/", "outputs/")
                with open(fp_out, "w+") as f:
                    json.dump(outputs, f)
                output_paths.append(fp_out)
                outputs = []

                if output_hidden:
                    fp_out = fp_prompts.replace(".json", f"-{idx}.pkl")
                    fp_out = fp_out.replace("inputs/", "outputs/")
                    with open(fp_out, "wb+") as f:
                        pickle.dump(hidden_states, f)
                    hidden_states = []

                idx += 1

        fp_out = fp_prompts.replace(".json", f"-{idx}.json")
        fp_out = fp_out.replace("inputs/", "outputs/")
        with open(fp_out, "w+") as f:
            json.dump(outputs, f)
        output_paths.append(fp_out)

        if output_hidden:
            fp_out = fp_prompts.replace(".json", f"-{idx}.pkl")
            fp_out = fp_out.replace("inputs/", "outputs/")
            with open(fp_out, "wb+") as f:
                pickle.dump(hidden_states, f)

        all_outputs = []
        for fp_out in output_paths:
            with open(fp_out) as f:
                all_outputs.extend(json.load(f))

        fp_out = fp_prompts.replace("inputs/", "outputs/")
        with open(fp_out, "w+") as f:
            json.dump(all_outputs, f)

        for fp_out in output_paths:
            os.system(f"rm {fp_out}")

