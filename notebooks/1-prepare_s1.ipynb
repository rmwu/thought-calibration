{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8ad1a0a-f373-4cc9-bb7f-52d252bc518e",
   "metadata": {},
   "source": [
    "# Prepare S1 dataset for training / calibration\n",
    "\n",
    "This notebook generates prompts to be run with `run_{model}.py` scripts. Goals include:\n",
    "- Budget forcing\n",
    "- Verify model results (produce labels for probes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70f0e9bc-87d7-4e9f-8ac2-5abc009e5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf84db65-a3ae-4f66-91f4-c6b138634946",
   "metadata": {},
   "source": [
    "Reminder to authenticate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41844f86-4e67-40a0-b87d-f5708935879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.system(\"export HF_TOKEN=''\")\n",
    "# os.system(\"export HF_HOME=''\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e077aa-d3fe-4d2e-bbf2-817d0c88a213",
   "metadata": {},
   "source": [
    "File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f15b9308-3486-4af3-8c44-46f240f84f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_DIR = \"../inputs\"  # you can customize\n",
    "\n",
    "# this should be updated with where your outputs are saved\n",
    "model_to_folder = {\n",
    "    \"qwen2.5\": \"../outputs\",\n",
    "    \"qwq\": \"../outputs-qwq\",\n",
    "    \"llama3.3\": \"../outputs-llama\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0183c7ea-5f84-43da-b3e4-30c2bdc99924",
   "metadata": {},
   "source": [
    "## Truncate and embed s1K thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b84ab8de-a8c4-4198-b26f-385b16802e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# im lazy this makes the dataset easier to read\n",
    "fp_s1 = \"cache/s1.json\"\n",
    "if not os.path.exists(fp_s1):\n",
    "    ds = load_dataset(\"simplescaling/s1K-1.1\", split=\"train\")\n",
    "    ds.to_json(fp_s1)\n",
    "\n",
    "with open(fp_s1) as f:\n",
    "    ds = []\n",
    "    for line in f:\n",
    "        ds.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e90637b4-6b2c-4716-a133-18db387b6ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_hf = {\n",
    "    \"qwen2.5\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n",
    "    \"llama3.3\": \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\",\n",
    "    \"qwq\": \"Qwen/QwQ-32B\"\n",
    "}\n",
    "tokenizers = {}\n",
    "for model, path in model_to_hf.items():\n",
    "    tokenizers[model] = AutoTokenizer.from_pretrained(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dde0ea1-25c8-46c2-952d-66dc15162288",
   "metadata": {},
   "source": [
    "Prepare per-step examples. Add chat template BEFORE prefilling `<think> ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d96d91f-4e9d-4af7-bd68-76ada9ba05fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_truncated_prompts(item, model, batch_size=10):\n",
    "    \"\"\"\n",
    "    item         (dict) single element\n",
    "    model        specifies chat template / tokenizer\n",
    "    batch_size   number of steps per prompt (subsampling)\n",
    "    \"\"\"\n",
    "    question = item[\"question\"]\n",
    "    steps = separate_steps(item[\"deepseek_thinking_trajectory\"])\n",
    "    # split into progressively longer prompts\n",
    "    new_prompts = []\n",
    "    for i in range(0, len(steps), batch_size):\n",
    "        thoughts = \"\\n\\n\".join(steps[:i+batch_size])\n",
    "        full_prompt = format_prompt(question, thoughts, model)\n",
    "        if full_prompt.count(\"<think>\\n\\n\") > 1:\n",
    "            full_prompt = full_prompt.replace(\"<think>\\n\\n\", \"\", 1)\n",
    "        new_prompts.append(full_prompt)\n",
    "    return new_prompts\n",
    "\n",
    "\n",
    "def separate_steps(thoughts, delims=[\"wait\", \"Wait\", \"but\", \"But\"]):\n",
    "    \"\"\"\n",
    "    Split thoughts into steps = chunks delimited by \"\\n\\n\" and specified terms\n",
    "\n",
    "    thoughts  S1 thoughts\n",
    "    \"\"\"\n",
    "    steps = [\"\"]\n",
    "    for line in thoughts.split(\"\\n\"):\n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        line = line + \"\\n\"\n",
    "        new_step = False\n",
    "        for s in delims:\n",
    "            if s in line:\n",
    "                new_step = True\n",
    "                break\n",
    "        if new_step:\n",
    "            steps.append(line)\n",
    "        else:\n",
    "            steps[-1] += line\n",
    "    steps = [s.strip() for s in steps]\n",
    "    return steps\n",
    "\n",
    "\n",
    "def format_prompt(question, thoughts, model):\n",
    "    \"\"\"\n",
    "    question  full question\n",
    "    thoughts  S1 thoughts\n",
    "    model     specifies chat template / tokenizer\n",
    "    \"\"\"\n",
    "    prompt = convert(f\"{question} Please reason step by step, and put your final answer within \\\\boxed{{}}.\", tokenizers[model])\n",
    "    full_prompt = f\"\"\"{prompt}\n",
    "\n",
    "<think>\n",
    "\n",
    "{thoughts}\n",
    "\n",
    "</think>\n",
    "\n",
    "Final Answer:\n",
    "\"\"\"\n",
    "    return full_prompt\n",
    "\n",
    "\n",
    "def convert(messages, tokenizer):\n",
    "    if type(messages) is str:\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": messages}\n",
    "        ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34d5be6f-efd1-42f5-95cb-1163a30d93c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step_limits(item, model):\n",
    "    \"\"\"\n",
    "    This is used to align embeddings with steps\n",
    "\n",
    "    item         (dict) single element\n",
    "    model        specifies chat template / tokenizer\n",
    "    \"\"\"\n",
    "    question = item[\"question\"]\n",
    "    steps = separate_steps(item[\"deepseek_thinking_trajectory\"])\n",
    "    # split into progressively longer prompts\n",
    "    limits = []\n",
    "    left = None\n",
    "    for i in range(len(steps) + 1):\n",
    "        thoughts = \"\\n\\n\".join(steps[:i])\n",
    "        # trim\n",
    "        full_prompt = format_prompt(question, thoughts, model).replace(\"\"\"\n",
    "</think>\n",
    "\n",
    "Final Answer:\"\n",
    "\"\"\", \"\")\n",
    "        tokens = tokenizers[model](full_prompt)[\"input_ids\"]\n",
    "        right = len(tokens)\n",
    "        if left is not None:\n",
    "            limits.append((left, right))\n",
    "        left = right\n",
    "    return limits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf9c924-551d-4962-a33b-63f92ef49fc3",
   "metadata": {},
   "source": [
    "Generate prompts for budget forcing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00c8e2cf-6c9e-45c9-9879-772eeffa2dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_to_hf:\n",
    "    all_prompts = []\n",
    "    batch_idx = []\n",
    "    idx_to_index = []\n",
    "    for i, item in enumerate(ds):\n",
    "        prompts = generate_truncated_prompts(item, model)\n",
    "        all_prompts.extend(prompts)\n",
    "        batch_idx.extend([i] * len(prompts))\n",
    "        idx_to_index.append([len(all_prompts) - len(prompts) + i for i in range(len(prompts))])\n",
    "    # save prompts first\n",
    "    with open(os.path.join(PROMPT_DIR, f\"s1_truncated_{model}.json\"), \"w\") as f:\n",
    "        json.dump(all_prompts, f)\n",
    "    # this takes longer\n",
    "    fp_metadata = f\"cache/s1_metadata_{model}.json\"\n",
    "    if not os.path.exists(fp_metadata):\n",
    "        limits = []\n",
    "        for item in tqdm(ds):\n",
    "            limits.append(get_step_limits(item, model))\n",
    "        with open(fp_metadata, \"w\") as f:\n",
    "            json.dump({\n",
    "                \"batch_idx\": batch_idx,\n",
    "                \"idx_to_index\": idx_to_index,\n",
    "                \"step_limits\": limits,\n",
    "            }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a120f075-cea9-4d96-aa59-4e5095f4adab",
   "metadata": {},
   "source": [
    "Generate prompts for embedding thoughts (only need to run once, through the full thought trajectory, since we truncate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e3d6ccd-401b-4351-9948-f6c6e10f517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in model_to_hf:\n",
    "    prompts_embed = []\n",
    "    for i, item in enumerate(ds):\n",
    "        prompts_embed.extend(generate_truncated_prompts(item, model, batch_size=1000))\n",
    "    assert len(prompts_embed) == len(ds)\n",
    "    with open(os.path.join(PROMPT_DIR, f\"s1_embed_{model}.json\"), \"w\") as f:\n",
    "        json.dump(prompts_embed, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4158996f-2f27-44de-8d10-e4ed3a2998aa",
   "metadata": {},
   "source": [
    "## Verifier prompts\n",
    "\n",
    "These prompts provide labels for *training* probes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fdbc20-b230-43cb-a3d8-7d5b745edbd5",
   "metadata": {},
   "source": [
    "### Supervised probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c119e99-a91c-4dee-9cf5-98d0cdcb07ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "grading_prompt = \"\"\"You are an AI assistant for grading a science problem. The user will provide you with the question itself, the correct answer, and the student's attempt. Your job is to judge whether the attempt is correct by comparing it with the correct answer. If the correct answer is a number or choice, there should be no ambiguity, and you should directly compare the answer and the final result. If the attempt is incomplete, you should mark it as wrong. If the correct answer involves going through the entire reasoning process, you should judge the result based on whether the reasoning process is correct, compared to correct answer.\n",
    "\n",
    "Do NOT try to solve the problem yourself. Only grade the attempt based on the correct answer.\n",
    "\n",
    "The user will provide the attempt and the correct answer in the following format:\n",
    "\n",
    "# Problem\n",
    "{problem}\n",
    "\n",
    "## Correct answer\n",
    "{solution}\n",
    "\n",
    "## Student attempt\n",
    "{attempt}\n",
    "\n",
    "Explain your reasoning concisely, and end your response on a new line with only \"Yes\" or \"No\" (without quotes).\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt_supervised(question, attempt, solution):\n",
    "    user_prompt = f\"## Problem\\n{question}\\n\\n## Correct answer\\n{solution}\\n\\n## Student attempt\\n{attempt}\\n\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": grading_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95bd9926-b2c2-4b88-9788-02bae7b5f900",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, folder in model_to_folder.items():\n",
    "    fp_outputs = f\"{folder}/s1_truncated_{model}.json\"\n",
    "    if not os.path.exists(fp_outputs):\n",
    "        continue\n",
    "    with open(fp_outputs) as f:\n",
    "        outputs = json.load(f)\n",
    "\n",
    "    all_prompts_verify = []\n",
    "    \n",
    "    questions = [ds[i][\"question\"] for i in batch_index]\n",
    "    answers = [ds[i][\"question\"] for i in batch_index]\n",
    "    \n",
    "    for question, attempt, answer in zip(questions, outputs, answers):\n",
    "        prompt = get_prompt_supervised(question, attempt, answer)\n",
    "        all_prompts_verify.append(prompt)\n",
    "    \n",
    "    with open(os.path.join(PROMPT_DIR, f\"s1_verify_{model}.json\"), \"w\") as f:\n",
    "        json.dump(all_prompts_verify, f)\n",
    "    \n",
    "    print(len(all_prompts_verify))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0f5c7a-11ea-4bc2-9d33-f5fce205739a",
   "metadata": {},
   "source": [
    "### Consistency probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "189e7e2f-0a38-4bcf-a6d3-4cb5420204e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grading_prompt = \"\"\"You are an AI assistant for grading a science problem. The user will provide you with the question itself and two student attempts. Your job is to judge whether the two students arrive at the same answer. If question asks for a single numerical answer, there should be no ambiguity, and you should directly compare the two answers. If the question asks for multiple parts, the two attempts are identical if only if all of the parts arrive at the same conclusion.\n",
    "\n",
    "Do NOT try to solve the problem yourself. Only grade whether the two attempts are the same.\n",
    "\n",
    "The user will provide the problem and two attempts in the following format:\n",
    "\n",
    "# Problem\n",
    "\n",
    "{problem}\n",
    "\n",
    "## Attempt 1\n",
    "\n",
    "{attempt1}\n",
    "\n",
    "## Attempt 2\n",
    "\n",
    "{attempt2}\n",
    "\n",
    "Explain your reasoning concisely, and end your response on a new line with only \"Yes\" or \"No\" (without quotes).\n",
    "\"\"\"\n",
    "\n",
    "def get_prompt_consistency(question, attempt1, attempt2):\n",
    "    user_prompt = f\"## Problem\\n{question}\\n\\n## Attempt 1\\n\\n{attempt1}\\n\\n## Attempt 2\\n\\n{attempt2}\\n\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": grading_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25164e10-ee1c-4754-9515-b6378f2adfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, folder in model_to_folder.items():\n",
    "    fp_outputs = f\"{folder}/s1_truncated_{model}.json\"\n",
    "    if not os.path.exists(fp_outputs):\n",
    "        continue\n",
    "    with open(fp_outputs) as f:\n",
    "        outputs = json.load(f)\n",
    "\n",
    "    all_prompts_consistent = []\n",
    "    \n",
    "    questions = [ds[i][\"question\"] for i in batch_index]\n",
    "    reference_attempts = [outputs[indices[-1]] for indices in idx_to_index]\n",
    "    \n",
    "    for question, attempt, answer in zip(questions, outputs, answers):\n",
    "        cur_index = len(all_prompts_consistent)\n",
    "        reference = reference_attempts[batch_index[cur_index]]\n",
    "        prompt = get_prompt_consistency(question, reference, attempt)\n",
    "        all_prompts_consistent.append(prompt)\n",
    "    \n",
    "    with open(os.path.join(PROMPT_DIR, f\"s1_consistent_{model}.json\"), \"w\") as f:\n",
    "        json.dump(all_prompts_consistent, f)\n",
    "    \n",
    "    print(len(all_prompts_consistent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28818cd7-4389-4885-a941-549145d0ee99",
   "metadata": {},
   "source": [
    "### Novelty probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5360b7a-d9e7-4ec1-9e8c-c0f8bc1c75a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_novelty_prompts(item, batch_size=1):\n",
    "    question = item[\"question\"]\n",
    "    solution = item[\"solution\"]\n",
    "    steps = separate_steps(item[\"deepseek_thinking_trajectory\"])\n",
    "    steps = [f\"## step {i+1}\\n{s}\" for i, s in enumerate(steps)]\n",
    "    # split into progressively longer prompts\n",
    "    new_prompts = []\n",
    "    # technically 1-2 is deterministic, but this is for quality control\n",
    "    for i in range(1, len(steps), batch_size):\n",
    "        thoughts = \"\\n\\n\".join(steps[:i+batch_size])\n",
    "        system_prompt = f\"\"\"You are an AI assistant for assessing the quality of logical reasoning. The user will provide you with the question and an incomplete attempt, consisting of a series of reasoning steps. Your job is to judge whether current step appears to provide additional information, compared to the previous steps. If the current step is correct and novel, it is useful. If the current step is wrong or redundant, then it is not useful.\n",
    "\n",
    "Do NOT try to solve the problem yourself. It does not matter if the attempt is not complete. Only comment on whether the current step is useful.\n",
    "\n",
    "The user will provide the problem and reasoning steps in the following format:\n",
    "\n",
    "# Problem\n",
    "{{ problem }}\n",
    "\n",
    "# Reasoning\n",
    "## step 1\n",
    "{{ reasoning step 1 }}\n",
    "\n",
    "## step 2\n",
    "{{ reasoning step 2 }}\n",
    "\n",
    "...\n",
    "\n",
    "## step k\n",
    "{{ reasoning step k }}\n",
    "\n",
    "...\n",
    "\n",
    "## current step\n",
    "{{ current reasoning step }}\n",
    "\"\"\"\n",
    "        user_prompt = f\"\"\"# Problem\n",
    "{question}\n",
    "\n",
    "# Reasoning\n",
    "{thoughts}\n",
    "\n",
    "Explain your reasoning, and end your response on a new line with only \"Yes\" if the current step provides new information or \"No\" otherwise (without quotes).\n",
    "\"\"\"\n",
    "\n",
    "        new_prompts.append([\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ])\n",
    "    return new_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07d28c6f-fb13-4a2a-a480-f147bea93ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_novel = []\n",
    "index_novel = []\n",
    "idx_to_index_novel = [[] for _ in range(1000)]\n",
    "\n",
    "for i, item in enumerate(ds):\n",
    "    prompts = generate_novelty_prompts(item, batch_size=1)\n",
    "    prompts_novel.extend(prompts)\n",
    "    index_novel.extend([i] * len(prompts))\n",
    "    idx_to_index_novel[i] = [len(all_prompts) - len(prompts) + i for i in range(len(prompts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e3a6217-709d-46fe-a96d-463c26d43d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(PROMPT_DIR, \"s1_novel.json\"), \"w\") as f:\n",
    "#     json.dump(prompts_novel, f)\n",
    "\n",
    "# fp_metadata_novel = \"cache/s1_metadata_step.json\"\n",
    "# with open(fp_metadata_novel, \"w\") as f:\n",
    "#     json.dump({\n",
    "#         \"batch_index\": index_novel,\n",
    "#         \"idx_to_index\": idx_to_index_novel,\n",
    "#     }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f85b73-130c-42cb-bd1a-28d573b7b642",
   "metadata": {},
   "source": [
    "### Leaf probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65db1397-f987-4315-b1b2-7c6c542635da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_leaf_prompts(item, batch_size=10):\n",
    "    question = item[\"question\"]\n",
    "    steps = separate_steps(item[\"deepseek_thinking_trajectory\"])\n",
    "    # split into progressively longer prompts\n",
    "    new_prompts = []\n",
    "    # technically 1-2 is deterministic, but this is for quality control\n",
    "    for i in range(1, len(steps), batch_size):\n",
    "        thoughts = steps[i].strip()\n",
    "        system_prompt = f\"\"\"You are an AI assistant for parsing LLM outputs. The user will provide you with the question and an intermediate reasoning step. Your job is to judge whether the given step contains an attempt at a final answer.\n",
    "\n",
    "Do NOT attempt to solve the problem yourself. It does not matter if the answer is correct. Only comment on whether an attempt has been made.\n",
    "\n",
    "The user will provide the problem and reasoning steps in the following format:\n",
    "\n",
    "# Problem\n",
    "\n",
    "{{ problem }}\n",
    "\n",
    "# Reasoning step\n",
    "\n",
    "{{ reasoning step }}\n",
    "\"\"\"\n",
    "        user_prompt = f\"\"\"# Problem\n",
    "\n",
    "{question}\n",
    "\n",
    "# Reasoning step\n",
    "\n",
    "{thoughts}\n",
    "\n",
    "Explain your reasoning, and end your response on a new line with only \"Yes\" or \"No\" indicating whether or the given step makes an attempt at providing the final answer.\n",
    "\"\"\"\n",
    "\n",
    "        new_prompts.append([\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ])\n",
    "    return new_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "becd2aba-a984-4e49-b61a-74d08d60882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_leaf = []\n",
    "index_leaf = []\n",
    "idx_to_index_leaf = [[] for _ in range(1000)]\n",
    "\n",
    "for i, item in enumerate(ds):\n",
    "    prompts = generate_leaf_prompts(item, batch_size=1)\n",
    "    prompts_leaf.extend(prompts)\n",
    "    index_leaf.extend([i] * len(prompts))\n",
    "    idx_to_index_leaf[i] = [len(all_prompts) - len(prompts) + i for i in range(len(prompts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1ff561c-1daf-4a8f-8851-cfc7445bc7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(PROMPT_DIR, \"s1_leaf.json\"), \"w\") as f:\n",
    "#     json.dump(prompts_leaf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95a94981-5939-4481-9404-f929562d8611",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert index_leaf == index_novel\n",
    "assert idx_to_index_leaf == idx_to_index_novel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
