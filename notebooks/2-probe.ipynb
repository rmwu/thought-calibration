{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34044911-108c-4d34-97e8-0b0d90aebee3",
   "metadata": {},
   "source": [
    "# Train probes based on s1K trajectories\n",
    "\n",
    "Pretrained probes may be downloaded [here](https://figshare.com/articles/dataset/s1K_calibrated_probes/29242328) (skip to `3-calibrate.ipynb`).\n",
    "\n",
    "The code and data for probe training are provided for reproducibility.\n",
    "The data required to re-train our probes can be found [here](https://figshare.com/articles/dataset/s1K_step_embeddings/29230682) and should be placed under `PROBE_DATA_DIR`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d40a3f2-1d6a-4e43-aed1-b98ac517932a",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70f0e9bc-87d7-4e9f-8ac2-5abc009e5626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from collections import Counter, deque\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# hi sklearn\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6453a4ac-cf14-4046-854e-b91286f68b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROBE_DATA_DIR = \"../probes/data\"  # LLM embeddings\n",
    "PROBE_DIR = \"../probes\"  # you can customize\n",
    "\n",
    "# this should be updated with where your outputs are saved\n",
    "model_to_folder = {\n",
    "    \"qwen2.5\": \"../outputs\",\n",
    "    \"qwq\": \"../outputs-qwq\",\n",
    "    \"llama3.3\": \"../outputs-llama\"\n",
    "}\n",
    "\n",
    "# you can modify code to loop through instead if you wish\n",
    "MODEL = \"qwen2.5\"    # qwen2.5|qwq|llama3.3\n",
    "MODE = \"supervised\"  # supervised|consistent|novel|leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1aee6f-75c2-4c48-b6f9-e2ceb3da71dd",
   "metadata": {},
   "source": [
    "Our s1K splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e0da56-4ff0-4ccb-b6d4-8304f3868e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = {\n",
    "    \"train\": range(500),\n",
    "    \"val\": range(500, 550),\n",
    "    \"test\": range(550, 1000)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc05233-7611-4497-90f0-b42870ca5818",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Steps to embeddings\n",
    "\n",
    "Extract mean embeddings for each step. This script takes a lot of memory and time (depending on your file system speed).\n",
    "\n",
    "`{PROBE_DATA_DIR}/{model}_embed_steps.pkl` can be produced by the `s1_embed_{model}` prompt from `1-prepare_s1.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912dca2e-96ba-482d-a0c7-2962d9ebc9b0",
   "metadata": {},
   "source": [
    "```\n",
    "for model in model_to_folder:\n",
    "    \n",
    "    with open(f\"cache/s1_metadata_{model}.json\") as f:\n",
    "        idx_to_step_limits = json.load(f)[\"step_limits\"]\n",
    "    \n",
    "    folder = model_to_folder[model]\n",
    "    fps = glob.glob(f\"{folder}/s1_embed_{model}-*.pkl\")\n",
    "    fps = sorted(fps, key=lambda fp: int(fp.rsplit(\"-\", 1)[1].split(\".\")[0]))\n",
    "    print(len(fps))\n",
    "    \n",
    "    fp_out = f\"embeddings/{model}_embed_steps.pkl\"\n",
    "\n",
    "    if not os.path.exists(fp_out):\n",
    "        step_embeddings = []\n",
    "        for fp in fps:\n",
    "            with open(fp, \"rb\") as f:\n",
    "                embeddings = pickle.load(f)\n",
    "                for ebd in embeddings:\n",
    "                    cur_idx = len(step_embeddings)\n",
    "                    cur_ebs = []\n",
    "                    segments = idx_to_step_limits[cur_idx]\n",
    "                    # need to +1 because determined from length of previous\n",
    "                    for left, right in segments:\n",
    "                        if len(ebd[left+1:right+1]) == 0:\n",
    "                            continue\n",
    "                        cur_ebs.append(np.mean(ebd[left+1:right+1], axis=0))\n",
    "                    step_embeddings.append(cur_ebs)\n",
    "                print(\"done with\", fp)\n",
    "\n",
    "        with open(fp_out, \"wb\") as f:\n",
    "            pickle.dump(step_embeddings, f)\n",
    "        print(fp_out)\n",
    "        \n",
    "        for fp in fps:\n",
    "            os.remove(fp)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd262e5-5826-4360-844c-2c017f61f3c3",
   "metadata": {},
   "source": [
    "## Parse probe labels\n",
    "\n",
    "This section parses outputs of the `s1_verify_...` prompts from `1-prepare_s1.ipynb` into `{PROBE_DATA_DIR}/labels-{mode}-{model}.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c56cf62-3614-484c-bf7f-0bab199913c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ambiguous(line):\n",
    "    for s in [\"incorrect\", \"incomplete\", \"inaccura\"]:\n",
    "        if s in line:\n",
    "            return 0\n",
    "    if \"yes\" in line.lower():\n",
    "        return 1\n",
    "    if \"no\" in line.lower():\n",
    "        return 0\n",
    "    return -1\n",
    "\n",
    "\n",
    "def parse_probe_labels(verify_outputs, return_outliers=False):\n",
    "    \"\"\"\n",
    "    verify_outputs  (list[str]) LLM verifier outputs\n",
    "    \"\"\"\n",
    "    verify_results = []\n",
    "    outliers = []\n",
    "    for line in verify_outputs:\n",
    "        # only look at final answer\n",
    "        result = line.strip().rsplit(\"\\n\", 1)\n",
    "        if len(result) < 2:\n",
    "            label = get_ambiguous(line)\n",
    "            verify_results.append(label)\n",
    "            if label < 0:\n",
    "                outliers.append(line)\n",
    "            continue\n",
    "        result = result[1].strip()\n",
    "        if \"yes\" in result.lower():\n",
    "            verify_results.append(1)\n",
    "        elif \"no\" in result.lower():\n",
    "            verify_results.append(0)\n",
    "        else:\n",
    "            label = get_ambiguous(line)\n",
    "            verify_results.append(label)\n",
    "            if label < 0:\n",
    "                outliers.append(line)\n",
    "    if return_outliers:\n",
    "        return verify_results, outliers\n",
    "    return verify_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af25fbdd-a008-4ae7-8420-db1139505dd0",
   "metadata": {},
   "source": [
    "Group inputs by original question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40fa171a-0cb4-446f-9f85-f2c25e44cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_linear_probe_data(verify_results, batch_index, idx_to_index):\n",
    "    \"\"\"\n",
    "    verify_results  (list[int])        output from parse_probe_labels\n",
    "    batch_index     (list[int])        [0, 0, 0, ..., 1, 1, ...] etc from metadata                           \n",
    "    idx_to_index    (list[list[int]])  [[0,1,2,3], [4,5,6], ...] etc from metadata\n",
    "    \"\"\"\n",
    "    num_problems = max(batch_index)\n",
    "    \n",
    "    xs = []\n",
    "    ys = []\n",
    "    # loop through problems\n",
    "    for i in range(num_problems + 1):\n",
    "        # loop through trajectories\n",
    "        keep_x = []\n",
    "        keep_y = []\n",
    "        for j in idx_to_index[i]:\n",
    "            if verify_results[j] < 0:\n",
    "                continue\n",
    "            keep_x.append(j)\n",
    "            keep_y.append(verify_results[j])\n",
    "        xs.append(keep_x)\n",
    "        ys.append(keep_y)\n",
    "    \n",
    "    return {\n",
    "        \"index\": xs,\n",
    "        \"label\": ys\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07682f9-6c25-4e3f-8f8a-7d623b7f7d10",
   "metadata": {},
   "source": [
    "## Train probe\n",
    "\n",
    "This section traines probes using `{PROBE_DATA_DIR}/{model}_embed_steps.pkl` (inputs) and `{PROBE_DATA_DIR}/labels-{mode}-{model}.json` (labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0497b6c-f233-4f50-b227-f6fa77158955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata(model, mode):\n",
    "    \"\"\"\n",
    "    mode  (str) supervised|consistent|novel|leaf\n",
    "    \"\"\"\n",
    "    if mode in [\"supervised\", \"consistent\"]:\n",
    "        fp_metadata = f\"cache/s1_metadata_{model}.json\"\n",
    "    else:\n",
    "        fp_metadata = f\"cache/s1_metadata_step.json\"\n",
    "    with open(fp_metadata) as f:\n",
    "        info = json.load(f)\n",
    "    return info\n",
    "\n",
    "\n",
    "def load_probe_inputs(model):\n",
    "    with open(os.path.join(PROBE_DATA_DIR, f\"{model}_embed_steps.pkl\"), \"rb\") as f:\n",
    "        reps = pickle.load(f)\n",
    "    return reps\n",
    "\n",
    "\n",
    "def load_probe_labels(model, mode=\"supervised\"):\n",
    "    \"\"\"\n",
    "    mode  (str) supervised|consistent|novel|leaf\n",
    "    \"\"\"\n",
    "    # novel and leaf labels are not dependent on model\n",
    "    if mode in [\"supervised\", \"consistent\"]:\n",
    "        fp_labels = f\"labels-{mode}-{model}.json\"\n",
    "    else:\n",
    "        fp_labels = f\"labels-{mode}.json\"\n",
    "    # load JSON\n",
    "    with open(os.path.join(PROBE_DATA_DIR, fp_labels)) as f:\n",
    "        labels = json.load(f)\n",
    "        index = labels[\"index\"]\n",
    "        label = labels[\"label\"]\n",
    "    assert len(index) == len(label)\n",
    "    # transform label to cumulative for supervised|consistent\n",
    "    # for calibration validity\n",
    "    if mode in [\"supervised\", \"consistent\"]:\n",
    "        to_cumulative(index, label)\n",
    "    return index, label\n",
    "\n",
    "def to_cumulative(index, label):\n",
    "    \"\"\"\n",
    "    applied for supervised and consistent probe.\n",
    "    modifies `label` and `index` in place.\n",
    "    \"\"\"\n",
    "    for i, lbl in enumerate(label):\n",
    "        if 1 not in lbl:\n",
    "            label[i] = []  # skip\n",
    "            index[i] = []\n",
    "            continue\n",
    "        first = lbl.index(1)\n",
    "        for j in range(first, len(lbl)):\n",
    "            lbl[j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "516a9241-1df7-4397-ba16-76b1a2496741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last(abs_idx, rel_idx, batch_size=10):\n",
    "    \"\"\"\n",
    "    get representation of last step\n",
    "    \n",
    "    batch_size  should match generate_truncated_prompts\n",
    "    \"\"\"\n",
    "    question_idx = batch_index[abs_idx]\n",
    "    last_rep = rel_idx * batch_size + batch_size\n",
    "    if last_rep >= len(reps[question_idx]):  # batch_size overshoots\n",
    "        last_rep = len(reps[question_idx]) - 1\n",
    "    return reps[question_idx][last_rep]\n",
    "\n",
    "\n",
    "def get_up_to_last(abs_idx, rel_idx, batch_size=10):\n",
    "    \"\"\"\n",
    "    get representations of all steps\n",
    "    \n",
    "    batch_size  should match generate_truncated_prompts\n",
    "    \"\"\"\n",
    "    question_idx = batch_index[abs_idx]\n",
    "    last_rep = rel_idx * batch_size + batch_size\n",
    "    return reps[question_idx][:last_rep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fb67fc-3102-4ff6-879f-3bdd827d97e8",
   "metadata": {},
   "source": [
    "### Supervised and consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8971fea0-c19d-4ed9-a60e-e09206e934a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = load_metadata(MODEL, MODE)\n",
    "batch_index = info[\"batch_idx\"]\n",
    "idx_to_index = info[\"idx_to_index\"]\n",
    "# Xs\n",
    "reps = load_probe_inputs(MODEL)\n",
    "# index of Xs, corresponding ys\n",
    "index, label = load_probe_labels(MODEL, MODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303b4a78-5a7b-478e-a9ca-ee00ae65d467",
   "metadata": {},
   "source": [
    "**Minor note**: Sometimes PCA hangs even with fixed `random_state`. Restart notebook and it should work O_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b635ee0-b8e0-4e3d-ad3c-9128cdf8da12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_probes = os.path.join(PROBE_DIR, f\"probe-{MODE}-{MODEL}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb261f17-8fcb-4d28-8027-e6ce1d3108b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2038 2038\n",
      "189 189\n",
      "200 200\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for i in splits[\"train\"]:\n",
    "    for j, idx in enumerate(index[i]):\n",
    "        X_train.append(get_last(idx, j))\n",
    "        y_train.append(label[i][j])\n",
    "\n",
    "print(len(X_train), len(y_train))\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "for i in splits[\"val\"]:\n",
    "    for j, idx in enumerate(index[i]):\n",
    "        X_val.append(get_last(idx, j))\n",
    "        y_val.append(label[i][j])\n",
    "\n",
    "print(len(X_val), len(y_val))\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for i in splits[\"test\"]:\n",
    "    if len(index[i]) == 0:\n",
    "        continue\n",
    "    np.random.seed(i)\n",
    "    j = np.random.choice(len(index[i]))\n",
    "    idx = index[i][j]\n",
    "    X_test.append(get_last(idx, j))\n",
    "    y_test.append(label[i][j])\n",
    "\n",
    "print(len(X_test), len(y_test))\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "if os.path.exists(fp_probes):\n",
    "    with open(fp_probes, \"rb\") as f:\n",
    "        lr, scaler, pca = pickle.load(f)\n",
    "else:\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "if not os.path.exists(fp_probes):\n",
    "    pca = PCA(n_components=256, random_state=0).fit(X_train)\n",
    "\n",
    "X_train = pca.transform(X_train)\n",
    "X_val = pca.transform(X_val)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "if not os.path.exists(fp_probes):\n",
    "    lr = LogisticRegression(max_iter=5000).fit(\n",
    "        X=X_train, y=y_train)\n",
    "    \n",
    "    with open(fp_probes, \"wb\") as f:\n",
    "        pickle.dump([lr, scaler, pca], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01ff95d0-217b-4418-bf28-b20f475a96a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9329294002156455\n",
      "0.7431823304034257\n",
      "0.7879735835940216\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = lr.predict_proba(X_train)\n",
    "print(roc_auc_score(y_train, y_train_pred[:,1]))\n",
    "\n",
    "y_val_pred = lr.predict_proba(X_val)\n",
    "print(roc_auc_score(y_val, y_val_pred[:,1]))\n",
    "\n",
    "y_test_pred = lr.predict_proba(X_test)\n",
    "print(roc_auc_score(y_test, y_test_pred[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9fb2c2-1131-458e-983d-d82144b7d441",
   "metadata": {},
   "source": [
    "### Novel and leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b687ec0b-e423-4e12-a2bf-cb90e8ca4a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODE = \"leaf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9effda3-cc72-4090-8663-0bbc3136baf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_probes = os.path.join(PROBE_DIR, f\"probe-{MODE}-{MODEL}.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b088eb31-be80-4eac-beb8-bb874dd5dd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = load_metadata(MODEL, MODE)\n",
    "batch_index = info[\"batch_idx\"]\n",
    "idx_to_index = info[\"idx_to_index\"]\n",
    "# Xs\n",
    "reps = load_probe_inputs(MODEL)\n",
    "# index of Xs, corresponding ys\n",
    "index, label = load_probe_labels(MODEL, MODE)\n",
    "\n",
    "splits_inv = {}\n",
    "for i in range(len(info[\"idx_to_index\"])):\n",
    "    for s in splits:\n",
    "        if i in splits[s]:\n",
    "            splits_inv[i] = s\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4b71df0-a8c6-4c53-bb5b-84025d6dc3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 31015\n",
      "val 2973\n",
      "test 450\n"
     ]
    }
   ],
   "source": [
    "Xs = {split: [] for split in splits}\n",
    "ys = {split: [] for split in splits}\n",
    "\n",
    "for idx, (cur_labels, cur_reps) in enumerate(zip(label, reps)):\n",
    "    cur_split = splits_inv[idx]\n",
    "    assert len(cur_labels) == len(cur_reps), (idx, len(cur_labels), len(cur_reps))\n",
    "\n",
    "    # slightly different data indexing\n",
    "    if MODE == \"leaf\":\n",
    "        # calibration must be exchangeable\n",
    "        if cur_split == \"test\":\n",
    "            if len(cur_labels) - cur_labels.count(-1) < 1:\n",
    "                continue\n",
    "            valid_leaves = [i for i, lbl in enumerate(cur_labels) if lbl >= 0]\n",
    "            np.random.seed(idx)\n",
    "            j = np.random.choice(valid_leaves)\n",
    "            Xs[cur_split].append(cur_reps[j])\n",
    "            ys[cur_split].append(cur_labels[j])\n",
    "            continue\n",
    "    \n",
    "        # otherwise add all\n",
    "        for i, (lbl, rep) in enumerate(zip(cur_labels, cur_reps)):\n",
    "            if lbl < 0:\n",
    "                continue\n",
    "            Xs[cur_split].append(rep)\n",
    "            ys[cur_split].append(lbl)\n",
    "\n",
    "    elif MODE == \"novel\":\n",
    "        # calibration must be exchangeable\n",
    "        if cur_split == \"test\":\n",
    "            if len(cur_labels) - cur_labels.count(-1) < 2:\n",
    "                continue\n",
    "            valid_scores = [i+1 for i, lbl in enumerate(cur_labels[1:]) if lbl >= 0]\n",
    "            np.random.seed(idx)\n",
    "            j = np.random.choice(valid_scores)\n",
    "            Xs[cur_split].append(np.concatenate([cur_reps[j], cur_reps[j-1]]))  # [i] is prev since we start from [1:]\n",
    "            ys[cur_split].append(cur_labels[j])\n",
    "            continue\n",
    "    \n",
    "        # otherwise add all\n",
    "        for i, (lbl, rep) in enumerate(zip(cur_labels[1:], cur_reps[1:])):\n",
    "            if lbl < 0:\n",
    "                continue\n",
    "            Xs[cur_split].append(np.concatenate([rep, cur_reps[i]]))  # [i] is prev since we start from [1:]\n",
    "            ys[cur_split].append(lbl)\n",
    "    \n",
    "    else:\n",
    "        raise Exception(\"Use the code above for other modes\")\n",
    "\n",
    "\n",
    "Xs = {split: np.array(X) for split, X in Xs.items()}\n",
    "ys = {split: np.array(y) for split, y in ys.items()}\n",
    "\n",
    "for split, X in Xs.items():\n",
    "    print(split, len(X))\n",
    "\n",
    "if os.path.exists(fp_probes):\n",
    "    with open(fp_probes, \"rb\") as f:\n",
    "        lr, scaler, pca = pickle.load(f)\n",
    "else:\n",
    "    scaler = StandardScaler().fit(Xs[\"train\"])\n",
    "\n",
    "for split, X in Xs.items():\n",
    "    Xs[split] = scaler.transform(X)\n",
    "\n",
    "if not os.path.exists(fp_probes):\n",
    "    pca = PCA(n_components=256, random_state=0).fit(Xs[\"train\"])\n",
    "\n",
    "for split, X in Xs.items():\n",
    "    Xs[split] = pca.transform(X)\n",
    "\n",
    "if not os.path.exists(fp_probes):\n",
    "    lr = LogisticRegression(max_iter=5000).fit(\n",
    "        X=Xs[\"train\"], y=ys[\"train\"])\n",
    "    \n",
    "    with open(fp_probes, \"wb\") as f:\n",
    "        pickle.dump([lr, scaler, pca], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b9078af-5acf-433f-b8b9-59639536423d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8680944749659607\n",
      "0.854880753664281\n",
      "0.8393614871691251\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = lr.predict_proba(Xs[\"train\"])\n",
    "print(roc_auc_score(ys[\"train\"], y_train_pred[:,1]))\n",
    "\n",
    "y_val_pred = lr.predict_proba(Xs[\"val\"])\n",
    "print(roc_auc_score(ys[\"val\"], y_val_pred[:,1]))\n",
    "\n",
    "y_test_pred = lr.predict_proba(Xs[\"test\"])\n",
    "print(roc_auc_score(ys[\"test\"], y_test_pred[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fed153f-9ea8-451c-a4be-07e597728c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
